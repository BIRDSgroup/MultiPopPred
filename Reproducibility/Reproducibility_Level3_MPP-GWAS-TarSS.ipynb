{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ae8a86b",
   "metadata": {},
   "source": [
    "# Load all libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b74f5fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import subprocess\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from statsmodels.formula.api import ols\n",
    "from pandas_plink import read_plink1_bin, read_plink"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3929e1",
   "metadata": {},
   "source": [
    "# Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cd95999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doMPP(X_tar, tar_snps, tar_corr, aux_betas, L1_penalty, L2_penalty):\n",
    "    \n",
    "    #Initialize Beta_hats\n",
    "    Beta_hat_init = np.random.normal(0, 0.00000000001, len(tar_snps))\n",
    "    \n",
    "    X_tensor = torch.from_numpy(X_tar)\n",
    "    XtX = torch.matmul(X_tensor.T, X_tensor)\n",
    "    XtX_np = XtX.numpy()\n",
    "\n",
    "    mu = 0.1\n",
    "    L1_penalty = L1_penalty\n",
    "    L2_penalty = L2_penalty\n",
    "\n",
    "    XtX_np = abs(1-L2_penalty)*XtX_np\n",
    "    r = tar_corr\n",
    "\n",
    "    def func(Bs, L1_penalty, L2_penalty):\n",
    "        t1 = torch.matmul(torch.from_numpy(Bs).T, XtX)\n",
    "        term1 = torch.matmul(t1, torch.from_numpy(Bs)).numpy()\n",
    "        term2 = 2*torch.matmul(torch.from_numpy(Bs).T, torch.from_numpy(r)).numpy()\n",
    "        term3 = L2_penalty*np.matmul(Bs.T,Bs)\n",
    "        temp = Bs - aux_betas\n",
    "        nesterov = mu*np.log((0.5*np.exp(-temp/mu))+(0.5*np.exp(temp/mu)))\n",
    "        term4 = L1_penalty*sum(nesterov)\n",
    "        return term1-term2+term3+term4\n",
    "\n",
    "    def jacfunc(Bs, L1_penalty, L2_penalty):\n",
    "        term1 = 2*torch.matmul(XtX,torch.from_numpy(Bs)).numpy()\n",
    "        term2 = 2*r\n",
    "        term3 = L2_penalty*2*Bs\n",
    "        temp = Bs - aux_betas\n",
    "        nesterov = np.divide((-np.exp(-temp/mu) + np.exp(temp/mu)),(np.exp(-temp/mu) + np.exp(temp/mu)))\n",
    "        term4 = L1_penalty*nesterov\n",
    "        return term1-term2+term3+term4\n",
    "\n",
    "    ans = sp.optimize.minimize(func, jac=jacfunc, x0=Beta_hat_init, args=(L1_penalty, L2_penalty),\\\n",
    "                               method='L-BFGS-B', options={'maxfun':10})\n",
    "    \n",
    "    final_snps = tar_snps\n",
    "    final_betas = ans.x\n",
    "    final_chr = tar_chroms\n",
    "    final_pos = tar_pos\n",
    "    final_a1 = tar_a1\n",
    "    final_results_df = pd.DataFrame({'CHR':final_chr, 'SNP':final_snps, 'POS':final_pos,\\\n",
    "                                     'A1':final_a1, 'BETA':final_betas,})\n",
    "    \n",
    "    return final_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bbcb69",
   "metadata": {},
   "source": [
    "# Change here to reproduce results on other simulation configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b83cf1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "inPATH = './Simul_Analysis_1/'\n",
    "\n",
    "aux_pops = 'EurEasAmrAfr'\n",
    "\n",
    "exp_list = ['exp1.4.1','exp1.4.2','exp1.4.3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf02418a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2025-08-09 11:48:22.485163\n",
      "End time: 2025-08-09 11:48:23.590493\n",
      "exp1.4.1 ... Done!\n",
      "Start time: 2025-08-09 11:48:46.874741\n",
      "End time: 2025-08-09 11:48:48.242642\n",
      "exp1.4.2 ... Done!\n",
      "Start time: 2025-08-09 11:49:11.987758\n",
      "End time: 2025-08-09 11:49:13.297667\n",
      "exp1.4.3 ... Done!\n"
     ]
    }
   ],
   "source": [
    "for exp_num in exp_list:\n",
    "    #AUX Summary Statistics\n",
    "    ss_df_aux1 = pd.read_csv(inPATH+'GWAS_SummaryStats/eur_'+str(exp_num)+'.sumstat', delimiter=' ', header=None,\\\n",
    "                             names = ['CHR','SNP','GENETIC.DIST','BP','A1','A2','BETA','SE','T','P','N'])\n",
    "    ss_df_aux2 = pd.read_csv(inPATH+'GWAS_SummaryStats/eas_'+str(exp_num)+'.sumstat', delimiter=' ', header=None,\\\n",
    "                             names = ['CHR','SNP','GENETIC.DIST','BP','A1','A2','BETA','SE','T','P','N'])\n",
    "    ss_df_aux3 = pd.read_csv(inPATH+'GWAS_SummaryStats/amr_'+str(exp_num)+'.sumstat', delimiter=' ', header=None,\\\n",
    "                             names = ['CHR','SNP','GENETIC.DIST','BP','A1','A2','BETA','SE','T','P','N'])\n",
    "    ss_df_aux4 = pd.read_csv(inPATH+'GWAS_SummaryStats/afr_'+str(exp_num)+'.sumstat', delimiter=' ', header=None,\\\n",
    "                             names = ['CHR','SNP','GENETIC.DIST','BP','A1','A2','BETA','SE','T','P','N'])\n",
    "\n",
    "    #TAR Summary Statistics\n",
    "    ss_df_tar = pd.read_csv(inPATH+'GWAS_SummaryStats/sas_'+str(exp_num)+'.sumstat', delimiter=' ', header=None,\\\n",
    "                            names = ['CHR','SNP','GENETIC.DIST','BP','A1','A2','BETA','SE','T','P','N'])\n",
    "\n",
    "    #External LD\n",
    "    G_sas  = read_plink1_bin(inPATH+'sas_chr22_geno_1KG.bed',\\\n",
    "                             inPATH+'sas_chr22_geno_1KG.bim',\\\n",
    "                             inPATH+'sas_chr22_geno_1KG.fam',\\\n",
    "                             verbose = False)\n",
    "    X_sas = G_sas.values\n",
    "    X_sas = np.where(X_sas == 2, 0, np.where(X_sas == 0, 2, X_sas))\n",
    "    X_sas = np.array(X_sas, dtype=float)\n",
    "    all_snps_sas_ld_full = G_sas.snp.values\n",
    "\n",
    "    common_snps = list(set(ss_df_aux1['SNP']) & set(ss_df_aux2['SNP']) &\\\n",
    "                       set(ss_df_aux3['SNP']) & set(ss_df_aux4['SNP']) &\\\n",
    "                       set(ss_df_tar['SNP']) & set(all_snps_sas_ld_full))\n",
    "\n",
    "    #Preprocess Target and Auxiliary Summ. Stats. File\n",
    "    ss_df_tar = ss_df_tar[ss_df_tar['SNP'].isin(common_snps)]\n",
    "    ss_df_aux1 = ss_df_aux1[ss_df_aux1['SNP'].isin(common_snps)]\n",
    "    ss_df_aux2 = ss_df_aux2[ss_df_aux2['SNP'].isin(common_snps)]\n",
    "    ss_df_aux3 = ss_df_aux3[ss_df_aux3['SNP'].isin(common_snps)]\n",
    "    ss_df_aux4 = ss_df_aux4[ss_df_aux4['SNP'].isin(common_snps)]\n",
    "\n",
    "    ss_df_tar = ss_df_tar.reset_index(drop=True)\n",
    "    ss_df_aux1 = ss_df_aux1.reset_index(drop=True)\n",
    "    ss_df_aux2 = ss_df_aux2.reset_index(drop=True)\n",
    "    ss_df_aux3 = ss_df_aux3.reset_index(drop=True)\n",
    "    ss_df_aux4 = ss_df_aux4.reset_index(drop=True)\n",
    "\n",
    "    N_tar = int(ss_df_tar['N'][0])\n",
    "    N_aux1 = int(ss_df_aux1['N'][0])\n",
    "    N_aux2 = int(ss_df_aux2['N'][0])\n",
    "    N_aux3 = int(ss_df_aux3['N'][0])\n",
    "    N_aux4 = int(ss_df_aux4['N'][0])\n",
    "\n",
    "    tar_snps = list(ss_df_tar['SNP'])\n",
    "    tar_a1 = list(ss_df_tar['A1'])\n",
    "    tar_pvals = np.asarray(list(ss_df_tar['P']), dtype='float')\n",
    "    tar_betas = np.asarray(list(ss_df_tar['BETA']), dtype='float')\n",
    "    tar_chroms = list(ss_df_tar['CHR'])\n",
    "    tar_pos = np.asarray(list(ss_df_tar['BP']), dtype='float')\n",
    "    tar_corr = np.asarray(list(ss_df_tar['BETA']), dtype='float')*N_tar\n",
    "\n",
    "    aux1_snps = list(ss_df_aux1['SNP'])\n",
    "    aux1_betas = np.asarray(list(ss_df_aux1['BETA']), dtype='float')\n",
    "\n",
    "    aux2_snps = list(ss_df_aux2['SNP'])\n",
    "    aux2_betas = np.asarray(list(ss_df_aux2['BETA']), dtype='float')\n",
    "\n",
    "    aux3_snps = list(ss_df_aux3['SNP'])\n",
    "    aux3_betas = np.asarray(list(ss_df_aux3['BETA']), dtype='float')\n",
    "\n",
    "    aux4_snps = list(ss_df_aux4['SNP'])\n",
    "    aux4_betas = np.asarray(list(ss_df_aux4['BETA']), dtype='float')\n",
    "\n",
    "    #Preprocess Target Ref. file\n",
    "    temp_set = set(tar_snps)\n",
    "    temp_indices_tar = [i for i, e in enumerate(all_snps_sas_ld_full) if e in temp_set]\n",
    "    all_snps_sas_ld_full = [i for i in all_snps_sas_ld_full if i in common_snps]\n",
    "    X_tar = X_sas[:, temp_indices_tar]\n",
    "\n",
    "    # Assign Weightage to each auxiliary population:\n",
    "    pops_tuple = aux_pops\n",
    "\n",
    "    if pops_tuple == 'EurOnly':\n",
    "        temp = (aux1_betas*1.0) + (aux2_betas*0.0) + (aux3_betas*0.0) + (aux4_betas*0.0)\n",
    "    elif pops_tuple == 'EasOnly':\n",
    "        temp = (aux1_betas*0.0) + (aux2_betas*1.0) + (aux3_betas*0.0) + (aux4_betas*0.0)\n",
    "    elif pops_tuple == 'AmrOnly':\n",
    "        temp = (aux1_betas*0.0) + (aux2_betas*0.0) + (aux3_betas*1.0) + (aux4_betas*0.0)\n",
    "    elif pops_tuple == 'AfrOnly':\n",
    "        temp = (aux1_betas*0.0) + (aux2_betas*0.0) + (aux3_betas*0.0) + (aux4_betas*1.0)\n",
    "\n",
    "    elif pops_tuple == 'EurAmr':\n",
    "        per_pop_weight = [0.5,0.5]\n",
    "        temp = (aux1_betas*per_pop_weight[0]) + (aux2_betas*0.0) +\\\n",
    "        (aux3_betas*per_pop_weight[1]) + (aux4_betas*0.0)\n",
    "    elif pops_tuple == 'EurEas':\n",
    "        per_pop_weight = [0.5,0.5]\n",
    "        temp = (aux1_betas*per_pop_weight[0]) + (aux2_betas*per_pop_weight[1]) +\\\n",
    "        (aux3_betas*0.0) + (aux4_betas*0.0)\n",
    "    elif pops_tuple == 'EurAfr':\n",
    "        per_pop_weight = [0.5,0.5]\n",
    "        temp = (aux1_betas*per_pop_weight[0]) + (aux2_betas*0.0) +\\\n",
    "        (aux3_betas*0.0) + (aux4_betas*per_pop_weight[1])\n",
    "    elif pops_tuple == 'EasAmr':\n",
    "        per_pop_weight = [0.5,0.5]\n",
    "        temp = (aux1_betas*0.0) + (aux2_betas*per_pop_weight[0]) +\\\n",
    "        (aux3_betas*per_pop_weight[1]) + (aux4_betas*0.0)\n",
    "    elif pops_tuple == 'EasAfr':\n",
    "        per_pop_weight = [0.5,0.5]\n",
    "        temp = (aux1_betas*0.0) + (aux2_betas*per_pop_weight[0]) +\\\n",
    "        (aux3_betas*0.0) + (aux4_betas*per_pop_weight[1])\n",
    "    elif pops_tuple == 'AmrAfr':\n",
    "        per_pop_weight = [0.5,0.5]\n",
    "        temp = (aux1_betas*0.0) + (aux2_betas*0.0) +\\\n",
    "        (aux3_betas*per_pop_weight[0]) + (aux4_betas*per_pop_weight[1])\n",
    "\n",
    "    elif pops_tuple == 'EurEasAmr':\n",
    "        per_pop_weight = [0.33,0.33,0.33]\n",
    "        temp = (aux1_betas*per_pop_weight[0]) + (aux2_betas*per_pop_weight[1]) +\\\n",
    "        (aux3_betas*per_pop_weight[2]) + (aux4_betas*0.0)\n",
    "    elif pops_tuple == 'EurEasAfr':\n",
    "        per_pop_weight = [0.33,0.33,0.33]\n",
    "        temp = (aux1_betas*per_pop_weight[0]) + (aux2_betas*per_pop_weight[1]) +\\\n",
    "        (aux3_betas*0.0) + (aux4_betas*per_pop_weight[2])\n",
    "    elif pops_tuple == 'EurAmrAfr':\n",
    "        per_pop_weight = [0.33,0.33,0.33]\n",
    "        temp = (aux1_betas*per_pop_weight[0]) + (aux2_betas*0.0) +\\\n",
    "        (aux3_betas*per_pop_weight[1]) + (aux4_betas*per_pop_weight[2])\n",
    "    elif pops_tuple == 'EasAmrAfr':\n",
    "        per_pop_weight = [0.33,0.33,0.33]\n",
    "        temp = (aux1_betas*0.0) + (aux2_betas*per_pop_weight[0]) +\\\n",
    "        (aux3_betas*per_pop_weight[1]) + (aux4_betas*per_pop_weight[2])\n",
    "\n",
    "    elif pops_tuple == 'EurEasAmrAfr':\n",
    "        per_pop_weight = [0.25,0.25,0.25,0.25]\n",
    "        temp = (aux1_betas*per_pop_weight[0]) + (aux2_betas*per_pop_weight[1]) + \\\n",
    "        (aux3_betas*per_pop_weight[2]) + (aux4_betas*per_pop_weight[3])\n",
    "    else:\n",
    "        print(\"Invalid Aux Pops!\")\n",
    "\n",
    "    aux_betas = temp\n",
    "\n",
    "    print('Start time: '+str(datetime.now()))\n",
    "    result_df = doMPP(X_tar, tar_snps, tar_corr, aux_betas, 100, 0.05)\n",
    "    print('End time: '+str(datetime.now()))\n",
    "    print(str(exp_num)+' ... Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7d0642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b02f0df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4614e850",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
