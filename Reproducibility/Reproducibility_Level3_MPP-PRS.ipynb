{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ae8a86b",
   "metadata": {},
   "source": [
    "# Load all libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b74f5fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import subprocess\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from statsmodels.formula.api import ols\n",
    "from pandas_plink import read_plink1_bin, read_plink"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3929e1",
   "metadata": {},
   "source": [
    "# Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c4c7a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doMPP(X_tar, Y_train_true, tar_snps, aux_betas, L1_penalty):\n",
    "    \n",
    "    #Initialize Beta_hats\n",
    "    Beta_hat_init = np.random.normal(0, 0.00000000001, len(tar_snps))\n",
    "    \n",
    "    X_tensor = torch.from_numpy(X_tar)\n",
    "    XtX = torch.matmul(X_tensor.T, X_tensor)\n",
    "    XtX_np = XtX.numpy()\n",
    "\n",
    "    y_tensor = torch.from_numpy(Y_train_true)\n",
    "    Xty = torch.matmul(X_tensor.T, y_tensor)\n",
    "    Xty_np = Xty.numpy()\n",
    "\n",
    "    mu = 0.1\n",
    "    L1_penalty = L1_penalty\n",
    "\n",
    "    def func(Bs, L1_penalty):\n",
    "        t1 = torch.matmul(X_tensor, torch.from_numpy(Bs)).numpy()\n",
    "        temp = Bs - aux_betas\n",
    "        nesterov = mu*np.log((0.5*np.exp(-temp/mu))+(0.5*np.exp(temp/mu)))\n",
    "        term2 = L1_penalty*sum(nesterov)\n",
    "        val = sum((Y_train_true - t1)**2) + term2\n",
    "        return val\n",
    "\n",
    "    def jacfunc(Bs, L1_penalty):\n",
    "        term1 = 2*torch.matmul(XtX, torch.from_numpy(Bs)).numpy()\n",
    "        term2 = 2*Xty_np\n",
    "        temp = Bs - aux_betas\n",
    "        nesterov = np.divide((-np.exp(-temp/mu) + np.exp(temp/mu)),(np.exp(-temp/mu) + np.exp(temp/mu)))\n",
    "        term3 = L1_penalty*nesterov\n",
    "        return term1-term2+term3\n",
    "\n",
    "    ans = sp.optimize.minimize(func, jac=jacfunc, x0=Beta_hat_init, args=(L1_penalty),\\\n",
    "                               method='L-BFGS-B', options={'maxfun':10})\n",
    "    \n",
    "    final_snps = tar_snps\n",
    "    final_betas = ans.x\n",
    "    final_a0 = tar_a0\n",
    "    final_a1 = tar_a1\n",
    "    final_results_df = pd.DataFrame({'SNP':final_snps, 'A0':final_a0,\\\n",
    "                                     'A1':final_a1, 'BETA':final_betas,})\n",
    "    \n",
    "    return final_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bbcb69",
   "metadata": {},
   "source": [
    "# Change here to reproduce results on other simulation configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6266e4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inPATH = './Simul_Analysis_1/'\n",
    "\n",
    "aux_pops = 'EurEasAmrAfr'\n",
    "\n",
    "exp_list = ['exp1.4.1','exp1.4.2','exp1.4.3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77ced004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2025-08-09 11:22:52.031523\n",
      "End time: 2025-08-09 11:22:52.237106\n",
      "exp1.4.1 ... Done!\n",
      "Start time: 2025-08-09 11:22:52.577076\n",
      "End time: 2025-08-09 11:22:52.648502\n",
      "exp1.4.2 ... Done!\n",
      "Start time: 2025-08-09 11:22:52.856572\n",
      "End time: 2025-08-09 11:22:52.873382\n",
      "exp1.4.3 ... Done!\n"
     ]
    }
   ],
   "source": [
    "for exp_num in exp_list:\n",
    "    #AUX Summary Statistics\n",
    "    ss_df_aux1 = pd.read_csv(inPATH+'Additional/Results_SOTAmethods/lassosumExtLD_pred_betas_eur_'+str(exp_num)+'.txt', delimiter='\\t', header=None,\\\n",
    "                             names = ['rsid','a1','a0','weight'])\n",
    "    ss_df_aux2 = pd.read_csv(inPATH+'Additional/Results_SOTAmethods/lassosumExtLD_pred_betas_eas_'+str(exp_num)+'.txt', delimiter='\\t', header=None,\\\n",
    "                             names = ['rsid','a1','a0','weight'])\n",
    "    ss_df_aux3 = pd.read_csv(inPATH+'Additional/Results_SOTAmethods/lassosumExtLD_pred_betas_amr_'+str(exp_num)+'.txt', delimiter='\\t', header=None,\\\n",
    "                             names = ['rsid','a1','a0','weight'])\n",
    "    ss_df_aux4 = pd.read_csv(inPATH+'Additional/Results_SOTAmethods/lassosumExtLD_pred_betas_afr_'+str(exp_num)+'.txt', delimiter='\\t', header=None,\\\n",
    "                             names = ['rsid','a1','a0','weight'])\n",
    "\n",
    "    #TAR Summary Statistics\n",
    "    ss_df_tar = pd.read_csv(inPATH+'Additional/Results_SOTAmethods/lassosumExtLD_pred_betas_sas_'+str(exp_num)+'.txt', delimiter='\\t', header=None,\\\n",
    "                             names = ['rsid','a1','a0','weight'])\n",
    "\n",
    "    #True X\n",
    "    G_sas  = read_plink1_bin(inPATH+'Genotypes/sas_train_'+str(exp_num)+'_CHR22.bed',\\\n",
    "                             inPATH+'Genotypes/sas_train_'+str(exp_num)+'_CHR22.bim',\\\n",
    "                             inPATH+'Genotypes/sas_train_'+str(exp_num)+'_CHR22.fam',\\\n",
    "                             verbose = False)\n",
    "    X_sas = G_sas.values\n",
    "    X_sas = np.where(X_sas == 2, 0, np.where(X_sas == 0, 2, X_sas))\n",
    "    X_sas = np.array(X_sas, dtype=float)\n",
    "    all_snps_sas_ld_full = G_sas.snp.values\n",
    "\n",
    "    common_snps = list(set(ss_df_aux1['rsid']) & set(ss_df_aux2['rsid']) &\\\n",
    "                       set(ss_df_aux3['rsid']) & set(ss_df_aux4['rsid']) &\\\n",
    "                       set(ss_df_tar['rsid']) & set(all_snps_sas_ld_full))\n",
    "\n",
    "    #Preprocess Target and Auxiliary Summ. Stats. File\n",
    "    ss_df_tar = ss_df_tar[ss_df_tar['rsid'].isin(common_snps)]\n",
    "    ss_df_aux1 = ss_df_aux1[ss_df_aux1['rsid'].isin(common_snps)]\n",
    "    ss_df_aux2 = ss_df_aux2[ss_df_aux2['rsid'].isin(common_snps)]\n",
    "    ss_df_aux3 = ss_df_aux3[ss_df_aux3['rsid'].isin(common_snps)]\n",
    "    ss_df_aux4 = ss_df_aux4[ss_df_aux4['rsid'].isin(common_snps)]\n",
    "\n",
    "    ss_df_tar = ss_df_tar.reset_index(drop=True)\n",
    "    ss_df_aux1 = ss_df_aux1.reset_index(drop=True)\n",
    "    ss_df_aux2 = ss_df_aux2.reset_index(drop=True)\n",
    "    ss_df_aux3 = ss_df_aux3.reset_index(drop=True)\n",
    "    ss_df_aux4 = ss_df_aux4.reset_index(drop=True)\n",
    "\n",
    "    tar_snps = list(ss_df_tar['rsid'])\n",
    "    tar_a1 = list(ss_df_tar['a1'])\n",
    "    tar_a0 = list(ss_df_tar['a0'])\n",
    "    tar_betas = np.asarray(list(ss_df_tar['weight']), dtype='float')\n",
    "\n",
    "    aux1_snps = list(ss_df_aux1['rsid'])\n",
    "    aux1_betas = np.asarray(list(ss_df_aux1['weight']), dtype='float')\n",
    "\n",
    "    aux2_snps = list(ss_df_aux2['rsid'])\n",
    "    aux2_betas = np.asarray(list(ss_df_aux2['weight']), dtype='float')\n",
    "\n",
    "    aux3_snps = list(ss_df_aux3['rsid'])\n",
    "    aux3_betas = np.asarray(list(ss_df_aux3['weight']), dtype='float')\n",
    "\n",
    "    aux4_snps = list(ss_df_aux4['rsid'])\n",
    "    aux4_betas = np.asarray(list(ss_df_aux4['weight']), dtype='float')\n",
    "\n",
    "    #Preprocess Target Ref. file\n",
    "    temp_set = set(tar_snps)\n",
    "    temp_indices_tar = [i for i, e in enumerate(all_snps_sas_ld_full) if e in temp_set]\n",
    "    all_snps_sas_ld_full = [i for i in all_snps_sas_ld_full if i in common_snps]\n",
    "    X_tar = X_sas[:, temp_indices_tar]\n",
    "\n",
    "    #True Y\n",
    "    Y_train_true = pd.read_csv(inPATH+'Phenotypes/pheno_sas_train_'+str(exp_num)+'.truepheno',\\\n",
    "                                   delimiter=' ', header=None, names=['Pheno'])\n",
    "    Y_train_true = np.array(Y_train_true['Pheno'], dtype=float)\n",
    "\n",
    "    #Assign Weightage to each auxiliary population:\n",
    "    pops_tuple = aux_pops\n",
    "\n",
    "    if pops_tuple == 'EurOnly':\n",
    "        temp = (aux1_betas*1.0) + (aux2_betas*0.0) + (aux3_betas*0.0) + (aux4_betas*0.0)\n",
    "    elif pops_tuple == 'EasOnly':\n",
    "        temp = (aux1_betas*0.0) + (aux2_betas*1.0) + (aux3_betas*0.0) + (aux4_betas*0.0)\n",
    "    elif pops_tuple == 'AmrOnly':\n",
    "        temp = (aux1_betas*0.0) + (aux2_betas*0.0) + (aux3_betas*1.0) + (aux4_betas*0.0)\n",
    "    elif pops_tuple == 'AfrOnly':\n",
    "        temp = (aux1_betas*0.0) + (aux2_betas*0.0) + (aux3_betas*0.0) + (aux4_betas*1.0)\n",
    "\n",
    "    elif pops_tuple == 'EurAmr':\n",
    "        per_pop_weight = [0.5,0.5]\n",
    "        temp = (aux1_betas*per_pop_weight[0]) + (aux2_betas*0.0) +\\\n",
    "        (aux3_betas*per_pop_weight[1]) + (aux4_betas*0.0)\n",
    "    elif pops_tuple == 'EurEas':\n",
    "        per_pop_weight = [0.5,0.5]\n",
    "        temp = (aux1_betas*per_pop_weight[0]) + (aux2_betas*per_pop_weight[1]) +\\\n",
    "        (aux3_betas*0.0) + (aux4_betas*0.0)\n",
    "    elif pops_tuple == 'EurAfr':\n",
    "        per_pop_weight = [0.5,0.5]\n",
    "        temp = (aux1_betas*per_pop_weight[0]) + (aux2_betas*0.0) +\\\n",
    "        (aux3_betas*0.0) + (aux4_betas*per_pop_weight[1])\n",
    "    elif pops_tuple == 'EasAmr':\n",
    "        per_pop_weight = [0.5,0.5]\n",
    "        temp = (aux1_betas*0.0) + (aux2_betas*per_pop_weight[0]) +\\\n",
    "        (aux3_betas*per_pop_weight[1]) + (aux4_betas*0.0)\n",
    "    elif pops_tuple == 'EasAfr':\n",
    "        per_pop_weight = [0.5,0.5]\n",
    "        temp = (aux1_betas*0.0) + (aux2_betas*per_pop_weight[0]) +\\\n",
    "        (aux3_betas*0.0) + (aux4_betas*per_pop_weight[1])\n",
    "    elif pops_tuple == 'AmrAfr':\n",
    "        per_pop_weight = [0.5,0.5]\n",
    "        temp = (aux1_betas*0.0) + (aux2_betas*0.0) +\\\n",
    "        (aux3_betas*per_pop_weight[0]) + (aux4_betas*per_pop_weight[1])\n",
    "\n",
    "    elif pops_tuple == 'EurEasAmr':\n",
    "        per_pop_weight = [0.33,0.33,0.33]\n",
    "        temp = (aux1_betas*per_pop_weight[0]) + (aux2_betas*per_pop_weight[1]) +\\\n",
    "        (aux3_betas*per_pop_weight[2]) + (aux4_betas*0.0)\n",
    "    elif pops_tuple == 'EurEasAfr':\n",
    "        per_pop_weight = [0.33,0.33,0.33]\n",
    "        temp = (aux1_betas*per_pop_weight[0]) + (aux2_betas*per_pop_weight[1]) +\\\n",
    "        (aux3_betas*0.0) + (aux4_betas*per_pop_weight[2])\n",
    "    elif pops_tuple == 'EurAmrAfr':\n",
    "        per_pop_weight = [0.33,0.33,0.33]\n",
    "        temp = (aux1_betas*per_pop_weight[0]) + (aux2_betas*0.0) +\\\n",
    "        (aux3_betas*per_pop_weight[1]) + (aux4_betas*per_pop_weight[2])\n",
    "    elif pops_tuple == 'EasAmrAfr':\n",
    "        per_pop_weight = [0.33,0.33,0.33]\n",
    "        temp = (aux1_betas*0.0) + (aux2_betas*per_pop_weight[0]) +\\\n",
    "        (aux3_betas*per_pop_weight[1]) + (aux4_betas*per_pop_weight[2])\n",
    "\n",
    "    elif pops_tuple == 'EurEasAmrAfr':\n",
    "        per_pop_weight = [0.25,0.25,0.25,0.25]\n",
    "        temp = (aux1_betas*per_pop_weight[0]) + (aux2_betas*per_pop_weight[1]) + \\\n",
    "        (aux3_betas*per_pop_weight[2]) + (aux4_betas*per_pop_weight[3])\n",
    "    else:\n",
    "        print(\"Invalid Aux Pops!\")\n",
    "\n",
    "    aux_betas = temp\n",
    "\n",
    "    print('Start time: '+str(datetime.now()))\n",
    "    result_df = doMPP(X_tar, Y_train_true, tar_snps, aux_betas, 7.5)\n",
    "    print('End time: '+str(datetime.now()))\n",
    "    print(str(exp_num)+' ... Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7d0642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b02f0df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4614e850",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
